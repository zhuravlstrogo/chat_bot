{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T16:50:19.936411Z","iopub.status.busy":"2023-11-20T16:50:19.935438Z","iopub.status.idle":"2023-11-20T16:51:28.211599Z","shell.execute_reply":"2023-11-20T16:51:28.210448Z","shell.execute_reply.started":"2023-11-20T16:50:19.936373Z"},"trusted":true},"outputs":[],"source":["%%capture\n","# !pip install accelerate==0.21.0 \\\n","#   bitsandbytes==0.40.2 \\\n","#   peft==0.5.0 \\\n","#   transformers==4.34.0 \\\n","#   sentencepiece \\\n","#     langchain \\\n","#      faiss-cpu \\\n","#     sentence-transformers\n","\n","# !pip install -U bitsandbytes \n","# !pip install -U transformers \n","# !pip install -U peft "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:47:36.408646Z","iopub.status.busy":"2023-11-23T07:47:36.408378Z","iopub.status.idle":"2023-11-23T07:50:38.097963Z","shell.execute_reply":"2023-11-23T07:50:38.096773Z","shell.execute_reply.started":"2023-11-23T07:47:36.408620Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install \"numpy>=1.12.1\"\n","!pip install \"scipy>=1.0.1\"\n","!pip install \"torch>=2.0.0\"\n","# !pip install transformers==4.34.0\n","!pip install transformers==4.35.2\n","!pip install accelerate==0.23.0\n","!pip install bitsandbytes==0.41.0\n","!pip install peft==0.5.0\n","!pip install pillow==10.0.1\n","# !pip install llama-cpp-python\n","# !pip install datasets\n","!pip install zstandard\n","!pip install jsonlines\n","!pip install sentencepiece\n","# !pip install fire\n","!pip install datasketch==1.5.9\n","!pip install nltk==3.8.1\n","# !pip install scikit-learn==1.3.0\n","!pip install pytest"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:50:38.101042Z","iopub.status.busy":"2023-11-23T07:50:38.100355Z","iopub.status.idle":"2023-11-23T07:50:38.150740Z","shell.execute_reply":"2023-11-23T07:50:38.149816Z","shell.execute_reply.started":"2023-11-23T07:50:38.101005Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:50:38.152202Z","iopub.status.busy":"2023-11-23T07:50:38.151901Z","iopub.status.idle":"2023-11-23T07:50:41.015160Z","shell.execute_reply":"2023-11-23T07:50:41.014252Z","shell.execute_reply.started":"2023-11-23T07:50:38.152178Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4.35.2\n"]}],"source":["import transformers\n","print(transformers.__version__)\n","# 4.35.2"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:54:30.239963Z","iopub.status.busy":"2023-11-23T07:54:30.239269Z","iopub.status.idle":"2023-11-23T07:54:34.315689Z","shell.execute_reply":"2023-11-23T07:54:34.314804Z","shell.execute_reply.started":"2023-11-23T07:54:30.239927Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import torch\n","# from langchain.document_loaders import DataFrameLoader\n","# from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# from langchain.embeddings import HuggingFaceEmbeddings\n","# from langchain.vectorstores import FAISS\n","import pandas as pd\n","from peft import PeftModel, PeftConfig\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n","import torch.nn.functional as F\n","\n","from typing import Any, List, Mapping, Optional\n","\n","# from langchain.callbacks.manager import CallbackManagerForLLMRun\n","# from langchain.llms.base import LLM"]},{"cell_type":"markdown","metadata":{},"source":["- [PEFT код](https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py#L264)\n","- [Модель Mistral_saiga_7b ток в точность fp16](https://huggingface.co/Gaivoronsky/Mistral-7B-Saiga)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:54:34.317510Z","iopub.status.busy":"2023-11-23T07:54:34.317062Z","iopub.status.idle":"2023-11-23T07:54:34.322800Z","shell.execute_reply":"2023-11-23T07:54:34.321860Z","shell.execute_reply.started":"2023-11-23T07:54:34.317481Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n"]}],"source":["\n","for d in {1:2, 3:4}.items():\n","    print()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:03:11.190361Z","iopub.status.busy":"2023-11-23T08:03:11.189390Z","iopub.status.idle":"2023-11-23T08:03:11.202571Z","shell.execute_reply":"2023-11-23T08:03:11.201494Z","shell.execute_reply.started":"2023-11-23T08:03:11.190307Z"},"trusted":true},"outputs":[],"source":["# MODEL_NAME = \"IlyaGusev/saiga_mistral_7b_lora\"\n","# MODEL_NAME = \"Gaivoronsky/Mistral-7B-Saiga\"\n","MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n","DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n","DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n","# DEFAULT_SYSTEM_PROMPT = \"Ты извлекаешь информацию из текста\"\n","DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Conversation:\n","    def __init__(\n","        self,\n","        message_template=DEFAULT_MESSAGE_TEMPLATE,\n","        system_prompt=DEFAULT_SYSTEM_PROMPT,\n","        start_token_id=1,\n","        bot_token_id=9225\n","    ):\n","        self.message_template = message_template\n","        self.start_token_id = start_token_id\n","        self.bot_token_id = bot_token_id\n","        self.messages = [{\n","            \"role\": \"system\",\n","            \"content\": system_prompt\n","        }]\n","\n","    def get_start_token_id(self):\n","        return self.start_token_id\n","\n","    def get_bot_token_id(self):\n","        return self.bot_token_id\n","\n","    def add_user_message(self, message):\n","        self.messages.append({\n","            \"role\": \"user\",\n","            \"content\": message\n","        })\n","\n","    def add_bot_message(self, message):\n","        self.messages.append({\n","            \"role\": \"bot\",\n","            \"content\": message\n","        })\n","\n","    def get_prompt(self, tokenizer):\n","        final_text = \"\"\n","        for message in self.messages:\n","            message_text = self.message_template.format(**message)\n","            final_text += message_text\n","        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n","        return final_text.strip()\n","\n","\n","\n","def generate(model, tokenizer, prompt, generation_config):\n","    print()\n","    data = tokenizer(prompt,\n","                     return_tensors=\"pt\",\n","                     add_special_tokens=False,\n","    #                      padding=True,\n","    #                     truncation=True\n","                    )\n","    #print(data)\n","    data = {k: v.to(device) for k, v in data.items()}\n","    \n","    output_ids = model.generate(\n","        **data,\n","        generation_config = generation_config\n","    #         remove_invalid_values = True\n","    )[0]\n","    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n","    output = tokenizer.decode(output_ids, skip_special_tokens=False)\n","    return output.strip()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:54:40.220386Z","iopub.status.busy":"2023-11-23T07:54:40.219715Z","iopub.status.idle":"2023-11-23T07:54:40.226441Z","shell.execute_reply":"2023-11-23T07:54:40.225365Z","shell.execute_reply.started":"2023-11-23T07:54:40.220343Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:54:41.480815Z","iopub.status.busy":"2023-11-23T07:54:41.479942Z","iopub.status.idle":"2023-11-23T07:56:35.989624Z","shell.execute_reply":"2023-11-23T07:56:35.988728Z","shell.execute_reply.started":"2023-11-23T07:54:41.480780Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a0645fea485419aa0eb63b85bf69513","version_major":2,"version_minor":0},"text/plain":["Downloading adapter_config.json:   0%|          | 0.00/476 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c31ad42bebd742cbad170bbaf476bb6b","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da7e112c5c0b4cf3893cfdccfcd1c175","version_major":2,"version_minor":0},"text/plain":["Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b153eeb0e6440c6ace16f218403d32e","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3b1c25720c74b7f9bb0e780834f8596","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dc354d41841460f9b74c5b47a2e8aae","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00aba92bb09c4e0b82b4d446b8ace239","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d335790ede9f4e62990b573136d47209","version_major":2,"version_minor":0},"text/plain":["Downloading generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3ff73357a0646a6ad473d2c3f8eb632","version_major":2,"version_minor":0},"text/plain":["Downloading adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e9ec85222a0404d8e6a3c3ba55ed26f","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/278 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a660ae4c8be4a29b2d87fe52eb02165","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41ba2da608bb4c31b9e65dd2900f4479","version_major":2,"version_minor":0},"text/plain":["Downloading added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df739862d82e432c87c06e1b78ecd1c5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/118 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0b1c24bb29f4a198df431915647b9d7","version_major":2,"version_minor":0},"text/plain":["Downloading generation_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 2,\n","  \"max_new_tokens\": 3584,\n","  \"no_repeat_ngram_size\": 15,\n","  \"pad_token_id\": 0,\n","  \"repetition_penalty\": 1.2,\n","  \"temperature\": 0.5,\n","  \"top_k\": 30,\n","  \"top_p\": 0.9\n","}\n","\n","Прошло времени 114.5029125213623\n"]}],"source":["import time\n","\n","st_time = time.time()\n","config = PeftConfig.from_pretrained(MODEL_NAME)\n","model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path,\n","    load_in_8bit = True,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","model = PeftModel.from_pretrained(\n","    model,\n","    MODEL_NAME,\n","    torch_dtype=torch.float16,\n","    is_trainable = True,\n","    device_map=\"auto\"\n",")\n","\n","\n","model.eval()\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n","generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n","print(generation_config)\n","print(f'Прошло времени {time.time() - st_time}')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:35.991510Z","iopub.status.busy":"2023-11-23T07:56:35.991213Z","iopub.status.idle":"2023-11-23T07:56:35.997844Z","shell.execute_reply":"2023-11-23T07:56:35.997009Z","shell.execute_reply.started":"2023-11-23T07:56:35.991485Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='TheBloke/Llama-2-7B-fp16', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:35.999414Z","iopub.status.busy":"2023-11-23T07:56:35.999072Z","iopub.status.idle":"2023-11-23T07:56:37.010670Z","shell.execute_reply":"2023-11-23T07:56:37.009310Z","shell.execute_reply.started":"2023-11-23T07:56:35.999389Z"},"trusted":true},"outputs":[],"source":["%ls"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:37.015124Z","iopub.status.busy":"2023-11-23T07:56:37.014304Z","iopub.status.idle":"2023-11-23T07:56:37.027797Z","shell.execute_reply":"2023-11-23T07:56:37.026840Z","shell.execute_reply.started":"2023-11-23T07:56:37.015088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 16777216 || all params: 6755192832 || trainable%: 0.24836028248556738\n"]}],"source":["print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:37.029221Z","iopub.status.busy":"2023-11-23T07:56:37.028918Z","iopub.status.idle":"2023-11-23T07:56:37.039156Z","shell.execute_reply":"2023-11-23T07:56:37.038283Z","shell.execute_reply.started":"2023-11-23T07:56:37.029196Z"},"trusted":true},"outputs":[],"source":["# generation_config.__delattr__(\"top_p\")\n","# generation_config"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:37.040741Z","iopub.status.busy":"2023-11-23T07:56:37.040435Z","iopub.status.idle":"2023-11-23T07:56:37.049383Z","shell.execute_reply":"2023-11-23T07:56:37.048488Z","shell.execute_reply.started":"2023-11-23T07:56:37.040716Z"},"trusted":true},"outputs":[],"source":["# generation_config.__delattr__(\"top_p\")\n","# generation_config"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:37.050992Z","iopub.status.busy":"2023-11-23T07:56:37.050476Z","iopub.status.idle":"2023-11-23T07:56:43.008263Z","shell.execute_reply":"2023-11-23T07:56:43.007312Z","shell.execute_reply.started":"2023-11-23T07:56:37.050965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<unk>\n","<unk>\n"]}],"source":["print(tokenizer.decode([0]))\n","print(tokenizer.unk_token)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T07:56:43.010362Z","iopub.status.busy":"2023-11-23T07:56:43.009610Z","iopub.status.idle":"2023-11-23T07:56:43.016903Z","shell.execute_reply":"2023-11-23T07:56:43.016026Z","shell.execute_reply.started":"2023-11-23T07:56:43.010300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 2,\n","  \"max_new_tokens\": 3584,\n","  \"no_repeat_ngram_size\": 15,\n","  \"pad_token_id\": 0,\n","  \"repetition_penalty\": 1.2,\n","  \"temperature\": 0.3,\n","  \"top_k\": 40,\n","  \"top_p\": 0.9,\n","  \"unk_token\": 0\n","}\n","\n"]}],"source":["# Изменение конфигурации\n","generation_config.temperature = 0.3\n","generation_config.no_repeat_ngram_size = 15\n","generation_config.do_sample = True\n","generation_config.top_k = 40\n","generation_config.top_p = 0.9\n","# generation_config.num_beams=1\n","generation_config.unk_token = 0\n","print(generation_config)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:03:16.238515Z","iopub.status.busy":"2023-11-23T08:03:16.238115Z","iopub.status.idle":"2023-11-23T08:03:16.245255Z","shell.execute_reply":"2023-11-23T08:03:16.244292Z","shell.execute_reply.started":"2023-11-23T08:03:16.238482Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n","Wall time: 10.5 µs\n"]}],"source":["%%time\n","def get_message(inputs):\n","    conversation = Conversation()\n","    conversation.add_user_message(inputs)\n","    prompt = conversation.get_prompt(tokenizer)\n","    print('Промт', '\\n', '*'*100)\n","    print(prompt)\n","    print('*'*100)\n","    output = generate(model, tokenizer, prompt, generation_config)\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","p = \"\"\"\n","\n","\"\"\"\n","get_message([p])"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:03:35.999543Z","iopub.status.busy":"2023-11-23T08:03:35.999163Z","iopub.status.idle":"2023-11-23T08:04:30.789729Z","shell.execute_reply":"2023-11-23T08:04:30.788693Z","shell.execute_reply.started":"2023-11-23T08:03:35.999512Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Промт \n"," ****************************************************************************************************\n","<s>system\n","Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.</s><s>user\n","сколько в среднем весит человек</s><s>bot\n","****************************************************************************************************\n","\n"]},{"data":{"text/plain":["'В среднем мужчины на Земле весят 82,4 кг (181 фунта), а женщины - 69,5 кг (153 фунтов). Однако эти цифры могут сильно отличаться от страны до страны или региона. Например, в США средний вес мужчин составляет около 90-97 килограммов (около 198-212 фунтов) при росте 175 см (5\\'9\"), тогда как у женщин это 75-80 кг (165-176 фунтов) при росте 160 см (5’3\"). В Японии же средняя масса взрослых людей составляет всего 58,5 кг (129 фунты) для мужчин и 52,2 кг (115 фунтов) для женщин.</s>'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["get_message('сколько в среднем весит человек')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T05:55:59.897589Z","iopub.status.busy":"2023-11-21T05:55:59.897226Z","iopub.status.idle":"2023-11-21T05:55:59.903286Z","shell.execute_reply":"2023-11-21T05:55:59.902409Z","shell.execute_reply.started":"2023-11-21T05:55:59.897562Z"},"trusted":true},"outputs":[],"source":["print(tokenizer.bos_token_id)\n","print(tokenizer.eos_token_id)\n","print(tokenizer.decode([1]))\n","print(tokenizer.decode([2]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
